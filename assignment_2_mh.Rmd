---
title: "Assignment 2"
author: "Marco Hinojosa"
date: '2022-10-01'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r,echo=FALSE,warning=FALSE,message=FALSE}

packageList <- c("knitr","Publish","ggplot2","tidyverse","readr", "survival","gtsummary","faraway","MASS","leaps","heatmap3",
    "ISLR2","glmnet",'e1071','class','pROC',"bdpv")
for(package in packageList){
  if(!require(package,character.only = TRUE)){
    install.packages(package);require(package,character.only = TRUE);}
    }

```
Chapter 4 
13A 

This question should be answered using the Weekly data set, which is part of the ISLR2 
package. This data is similar in nature to the Smarket data from this chapterâ€™s lab, 
except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to 
the end of 2010. 

```{r}
data(Weekly)
```


(a) Produce some numerical and graphical summaries of the Weekly data. Do there 
appear to be any patterns? 
**There is some correlation with year and volume, otherwise not much else is apparent.** 

```{r}
head(Weekly)

table(Weekly$Year)

summary(Weekly)

# correlations between the predictors using cor() and use [,-9] to drop the 9th column (direction) 
# since it is not numerical can not be used by cor(). 
cor(Weekly[,-9])


```


(b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print 
the results. 

Do any of the predictors appear to be statistically significant? If so, which 
ones? 
**Lag2 p value < 0.05 appears to be significant**

```{r}

glmfit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data=Weekly, family = 'binomial')
summary(glmfit) 

# Alternative to above: using gtsummary package function tbl_regression can display some of the summary() results
# in a cleaner format.
glmfit %>% tbl_regression(exponentiate = TRUE)

```


(c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression. 
**The model is not any better than just uniformed prediction or guess as accuracy is 0.561 and uniformed accuracy would be 0.555.** 

```{r}
# add a column to the Weekly data with the predicted probability of it stock market going up using the above made model "glmfit". Will call this colum glm_prob_up. The type="response" option tells R to output probabilities of the form P(Y = 1|X), as opposed to other information such as the logit.
Weekly$glm_prob_up <- predict(glmfit,type='response')

# to be able to compare the models expected marked direction against the observed marked direction
# in the data set column direction will add another colum called glm_pred and populate it with either up/down string based upon the probabilities just added above. Will use the case_when() to fill this in with "Up" when the probability is greater thanor equal to 0.5 and otherwise just fill in "Down"
Weekly$glm_pred <- case_when(
  Weekly$glm_prob_up >= 0.5 ~ "Up",
  TRUE ~ "Down"
)

#now to make the confusion matrix will use base R function table() with the two columns of interest our predicted direction in glm_pred and the observed or true direction in Direction. 
table(Weekly$glm_pred, Weekly$Direction)

# alternatively could use caret package confusionMatrix().  Note that I had to add in the
# as.factor() when calling the glm_pred column because as filled in above it was designated a 
# string not automatically recognized as a factor.  
caret::confusionMatrix(data = as.factor(Weekly$glm_pred), reference = Weekly$Direction)

hist(Weekly$probUp)

table(Weekly$Direction)

ggplot(Weekly,aes(Direction,probUp))+geom_boxplot()

```


 
(d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag1 as the only predictor. Compute the confusion matrix and the overall fraction of  correct predictions for the held out data (that is, the data from 2009 and 2010). 

```{r}
#create the model with a subset of the data as requested
glmFit2008 <- glm(Direction ~ Lag1, data=subset(Weekly,Year<=2008), family = 'binomial')

#add predicted probabilities of market going up based on the new model above
Weekly$glm_prob_2008 <- predict(glmFit2008,newdata = Weekly)

#explained in section above
Weekly$glm_pred_2008 <- as.factor(case_when(
  Weekly$glm_prob_2008 >= 0.5 ~ "Up",
  TRUE ~ "Down"
))

#subset the held out data year > 2008. 
Weekly_9_10 <- Weekly %>% 
  filter(Year > 2008)

#explained in section above
caret::confusionMatrix(data = Weekly_9_10$glm_pred_2008, reference = Weekly_9_10$Direction)

```



(e) Repeat (d) using LDA. 

```{r}

training_set <- subset(Weekly,Year<=2008)

test_set <- subset(Weekly,Year>2008)

lda_fit_2008 <- lda(Direction ~ Lag1,data= training_set )

#Of note changed the newdata in this line from Weekly to test_set. At this point we want to 
#make a prediction of the probility of up or down direction of the market in the years in the test
# set 09/10.  We want to make that prediction using the model we fit on the training set. That #model is called lda_fit_2008. It would work just as well if we left newdata=Weekly as originally #written however this creeates predictions for the whole dataset including the training years which #we do not care about for the purpose of this question.  If we do it that way the length of the #test set and the length of the prediction class vector are different and will be a problem for our #table or confusionMatrix functions below. By setting newdata as test_set this is avoided.
lda_2008 <- predict(lda_fit_2008,newdata = test_set)

#shows that the lda_2008 object we just created has three elements whose names are 
#class, posterior, and x. The first element class is the LDA's prediction about the direction 
# the stock market will go.  The second element, posterior, is a matrix whose kth column contains #the posterior probability that the corresponding observation belongs to the kth class, computed 
#from bayes theorem. Finally, x contains the linear discriminants.
names(lda_2008)

head(lda_2008$posterior)

table(lda_2008$class, test_set$Direction)

caret::confusionMatrix(data = lda_2008$class, reference = test_set$Direction)

```



(f) Repeat (d) using QDA. 



```{r}
qda_fit_2008 <- qda(Direction ~ Lag1, data = training_set)

qda_2008 <- predict(qda_fit_2008, newdata = test_set)

caret::confusionMatrix(data = qda_2008$class, reference = test_set$Direction)
```

(g) Repeat (d) using KNN with K = 1. 

```{r}

trainingSet.X <- subset(training_set,select=c("Lag1"))

trainingSet.Y <- training_set$Direction

testSet.X <- subset(test_set,select=c("Lag1"))

knnFit <- knn(trainingSet.X,testSet.X,trainingSet.Y,k=1,prob = TRUE)

table(knnFit,test_set$Direction) %>% addmargins()

```



(h) Repeat (d) using naive Bayes. 


```{r}

nb2008 <- naiveBayes(Direction ~ Lag1, data=training_set)

preNb2008 <- predict(lda_fit_2008,newdata = test_set)$class

mean(preNb2008==test_set$Direction)

```



(i) Which of these methods appears to provide the best results on this data?


# Let's graph the AUC

```{r}


lda_fit_2008 <- lda(Direction ~ Lag1, data = training_set)

predLda2008 <- predict(lda_fit_2008,newdata = test_set)


Ytrue <- 1*(test_set$Direction=='Up')
pred <- predLda2008$posterior[,2]
rocPlot <- roc(Ytrue ~pred) 
auc<- rocPlot$auc

plot(rocPlot)
title(paste("AUC",round(auc,3)))



```

# Let's examine the confusion matrix

```{r}

confusion_matrix <- matrix(table(knnFit,test_set$Direction),2,2)

rownames(confusion_matrix) <- NULL
colnames(confusion_matrix) <- NULL


bdpv::BDtest(confusion_matrix, pr=0.1, conf.level = 0.95)
```

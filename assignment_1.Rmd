---
title: "Assignment 1"
author: "Marco Hinojosa"
date: '2022-08-24'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r,echo=FALSE,warning=FALSE,message=FALSE}

packageList <- c("knitr","Publish","ggplot2","tidyverse","readr", "survival","gtsummary","faraway","MASS","leaps","heatmap3",
    "ISLR2","glmnet")
for(package in packageList){
  if(!require(package,character.only = TRUE)){
    install.packages(package);require(package,character.only = TRUE);}
    }
library(ISLR2)
library(dplyr)
library(stats)
library(gtsummary)
library(ggplot2)

```
# Answer Chapter 2, question 9 in ISLR

**Load Auto data**

```{r}
data("Auto")
```

## Question 9 
**a) Which of the predictors are quantitative and which are qualitative?**

* qualitative: origin and name
* quantitative: mpg, cylinders, displacement, horsepower, weight, acceleration, year  

***

**b) What is the range of each quantitative predictor?**  

```{r echo=TRUE}
#summary function is quick way to see the range (min and max) as below. 
summary(Auto)

# alternatively to just get the min and max in a table could do something like:
 
# This creates a dataframe called r_min from the Auto dataframe, then uses the summarise function. Without the 
# across() function summarise would apply the 'min' function to the whole dataframe.
#  By using the across() function it tells summarise to apply the function to each
# column. Next repeat the process creating r_max.  Then create Auto_ranges dataframe
# putting together the two(r_min and r_max) using bind_rows(). Finally add a column 
# labeled 'range' using mutate() and filling in the values with a simple vector c().

{
r_min <- Auto %>%
    summarise(across(mpg:year, min))
r_max <- Auto %>%
    summarise(across(mpg:year, max))
Auto_ranges <- bind_rows(r_min, r_max) %>%
    mutate(range = c("minimum", "maximum"))
print(Auto_ranges)
}

```
***
**c) What is the mean and standard deviation of each quantitative predictor?**  

```{r}
#There is a conflict when using both dplyr and MASS packages.
#specify dplyr::select when both are loaded to resolve the conflict.

# take the Auto dataframe and select only the columns from mpg through year
# then pass this to the tbl_summary function. The 'type' argument specifies 
# the variable type (ex continuous, categorical, etc).  The function will default 
# to an appropriate type however here we wish to override the default regarding
#  our data for cylinders to make sure it is classified as continuous. The next
# argument in the tbl_summary function is 'statistic' and we ask it to find the 
# mean and standard deviations for all continuous variables.

 Auto %>%
  dplyr::select(mpg:year) %>%
  tbl_summary(type=list(cylinders~"continuous"),
              statistic = list(all_continuous() ~ "{mean},{sd}"))


```
***
**d) Now remove the 10th through 85th observations. What is the**
**range, mean, and standard deviation of each predictor in the**
**subset of the data that remains?**  

```{r}
# Select() only the columns of interest. Slice() only the rows of interest. Use
# tbl_summary function as above adding the min and max functions to be calculated
# as well. 
Auto %>%
    dplyr::select(mpg:year) %>%
    slice(c(1:9, 86:392)) %>% 
    tbl_summary(type=list(cylinders~"continuous"),
                statistic = list(all_continuous() ~ "{min}, {max},{mean},{sd}"))

```

***
**(e) Using the full data set, investigate the predictors graphically,**
**using scatterplots or other tools of your choice. Create some plots**
**highlighting the relationships among the predictors. Comment**
**on your findings.**  

* Introduction to ggplot2 including downloadable cheatsheet and tutorials:
* https://ggplot2.tidyverse.org


```{r}

# mpg vs weight/origin/cylinder/acceleration/year
# acceleration vs cylinder/displacement/weight/origin

ggplot(data = Auto, aes(x=year, y=mpg, color = cylinders)) + geom_point()  + facet_wrap(vars(origin)) + labs(title = "mpg by year", subtitle = "comparing country of origin")

ggplot(data = Auto, aes(x=year, y=acceleration, color = weight)) + geom_point()  + facet_grid(cols=vars(cylinders), rows = vars(origin)) + labs(title = "acceleration by year", subtitle = "comparing cylinders and country of origin")

ggplot(data = Auto, aes(x=weight, y=acceleration, color = displacement)) + geom_point() + labs(title = "acceleration by weight")

ggplot(data = Auto, aes(x=horsepower, y=acceleration, color = mpg)) + geom_point() + labs(title = "acceleration by horsepower")

ggplot(data = Auto, aes(x=weight, y=horsepower, color = displacement)) + geom_point() + facet_wrap(vars(cylinders)) + labs(title = "horsepower by weight", subtitle = "comparing by cylinders")

```

***
**(f) Suppose that we wish to predict gas mileage (mpg) on the basis**
**of the other variables. Do your plots suggest that any of the**
**other variables might be useful in predicting mpg? Justify your**
**answer.**  

* Weight, cylinders, displacement, horsepower all seem inversely related to mpg. 
* year, acceleration, and perhaps origin appear to be directly related to mpg.

```{r}
ggplot(data = Auto, aes(x=acceleration, y=mpg, color = year)) + geom_point() + facet_wrap(vars(origin)) + labs(title = "mpg by acceleration", subtitle = "comparing country of origin")
```


**Just FYI:**  

```{r}

# Create a 'make' variable by removing first part of name
# add a column named 'make' by applying the function gsub to the current name 
# column observations.  gsub() looks for a pattern of a character string and 
# replaces with something else you specify. The first argument is the pattern
# here specified as a space followed by anything (.* meaning wildcard).  The second 
# argument replaces it in this case with nothing (the empty quotes ""), the 
# last argument of the function is the data or x (Auto$name column). 
Auto$make <- gsub(" .*","",Auto$name)

# Create a table from the make colum and sort
table(Auto$make) %>% sort()

# Create a column 'Ford01' from the table above and fill in the Ford01 col
# with either a 1 or 0 using the ifelse(). 
Auto$Ford01 <- ifelse(Auto$make=='ford',1,0)

ggplot(Auto,aes(mpg,Ford01))+geom_point()+geom_smooth()

```
***
# just FYI: how to use LASSO with real example
# Use Lasso to predict Ford

```{r}

Xmatrix <- as.matrix(subset(Auto,select=c("mpg","cylinders",
                                      "horsepower","weight","acceleration","year")))

# The function cv.glmnet (cross validation generalized linear model with a net think
# fishing net) is like a lasso function. First argument is x (variables as noted above), second argument 
# y (response chosen here as Ford01, supported ('...' = not required) argument 'family' specified the response # variable as a binomial.  The fourth argument is type.measure specifies which loss equation to use.
# See cv.glmnet help file for details but apparently 5 options and though not all available depending on the
# model and in this example "auc" is for two-class logistic regression only and gives area under the 
# reciever operator curve. 
 
lassoFit <- cv.glmnet(x=Xmatrix,
                      y=Auto$Ford01,
              family="binomial",type.measure="auc")

plot(lassoFit)

glm(Auto$Ford01~Xmatrix,family='binomial') %>% summary()

```

***
# Exercise 10

(a) To begin, load in the Boston data set. The Boston data set is
part of the ISLR2 library.

Read about the data set:
> ?Boston
How many rows are in this data set? How many columns? What
do the rows and columns represent?

* 506 rows  *Each represents a suburb of Boston, Massachusetts.* 
* 13 columns

Column Name  | Description
------------- | -------------
crim    | per capita crime rate by town.
zn      | proportion of residential land zoned for lots over 25,000 sq.ft.
indus   | proportion of non-retail business acres per town.
chas    | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
nox     | nitrogen oxides concentration (parts per 10 million).
rm      | average number of rooms per dwelling.
age     | proportion of owner-occupied units built prior to 1940.
dis     | weighted mean of distances to five Boston employment centres.
rad     | index of accessibility to radial highways.
tax     | full-value property-tax rate per $10,000.
ptratio | pupil-teacher ratio by town.
lstat   | lower status of the population (percent).
medv    | median value of owner-occupied homes in $1000s.


***
(b) Make some pairwise scatterplots of the predictors (columns) in
this data set. Describe your findings.  

```{r}

data("Boston")
plot(Boston)


```


***
(c) Are any of the predictors associated with per capita crime rate?
If so, explain the relationship.  

> The plots above seem to show crime rate seems to be concentrated in certain suburbs where there is lower median values, higher percent of lower status of the population, more buildings built <1940, with closer proximity to the five Boston employment centers, and some relationship around industry/NO concentration.

```{r}
ggplot(data = Boston, aes(x=medv, y=crim)) + geom_point() + labs(title = "Crime Rate and Median Value")
ggplot(data = Boston, aes(x=lstat, y=crim)) + geom_point() + labs(title = "Crime Rate and % Lower Social-Economic Status")
ggplot(data = Boston, aes(x=age, y=crim)) + geom_point() + labs(title = "Crime Rate and Old Building %")
ggplot(data = Boston, aes(x=dis, y=crim)) + geom_point() + labs(title = "Crime Rate and Proximity to City Centers")   
ggplot(data = Boston, aes(x=nox, y=crim)) + geom_point() + labs(title = "Crime Rate and Nitroous Oxide Levels")
```


***
(d) Do any of the census tracts of Boston appear to have particularly
high crime rates? *yes* Tax rates? *yes* Pupil-teacher ratios? *no* Comment on
the range of each predictor. 

```{r}
summary(Boston)

```


***
(e) How many of the census tracts in this data set bound the Charles
river? *35*
```{r}
foo<- Boston %>% filter(chas == 1) %>% count()
print(foo)

```

***
(f) What is the median pupil-teacher ratio among the towns in this
data set? *19.05*

***
(g) Which census tract of Boston has lowest median value of owner occupied
homes? *The Boston data set does not contain a key or identifier however there are two rows in which the median value is tied for minimum (5)*
What are the values of the other predictors for that census tract, and how do those values compare to the
overall ranges for those predictors? Comment on your findings.

```{r}
low_medv <- Boston %>% filter(medv==(min(medv)))
print(low_medv)
```
These two rows show elevated crime rate, low percent of lots <25k ^2 ft, 3rd quartile industrial percentage, elevated NOX, near median rm, max % of buildings older than 1940, near min distance to city centers, high taxation, high student teacher ratio, above 3rd quartile lower socio-economic status percentage.

***
(h) In this data set, how many of the census tracts average more
than seven rooms per dwelling? *64*  More than eight rooms per
dwelling? *13* Comment on the census tracts that average more than
eight rooms per dwelling. *These all have medv well above 3rd quartile.*

```{r}
foo<- Boston %>% filter(rm > 7) %>% count()
print(foo)
foo<- Boston %>% filter(rm > 8) %>% count()
print(foo)

plus8_rm <- Boston %>% filter(rm > 8)
print(plus8_rm)

```


***
# Chapter 3

15A This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict lstat using the other variables in this data set. In other words, lstat is the response, and the other variables are the predictors.

(a)	For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.



```{r}
# for each of these variables create a model using lm polynomial function,  
# use the apply function to automate this to each variable.  Apply() has first argument
# as data, second argument "MARGIN" set here to '2' specifies to apply the function by column (can be 1 or 2 or both c1, 2).  The last argument is a function.  It will take each column and apply the function
# which in this case is a linear model lm().  lm() takes first argument model (here is y ~ x or response variable ~ predictor variable). 

lm_bost_df <- apply(Boston, 2, function(yy) summary(lm(lstat ~ yy, data = Boston)))
print(lm_bost_df)
```

^ All appear to have p values < 0.05 apart from the 'chas' variable or bordering the Charles River.

```{r}
sig_cols <- colnames(Boston)
sig_cols <- sig_cols[!sig_cols %in% c("lstat", "chas")]
Bost_abr <- Boston %>% dplyr::select(c(crim:indus, nox:medv))
apply(Bost_abr, 2, function(ii) {ggplot(data = Boston, aes(x=ii, y=lstat)) + geom_point() + labs(title = paste0("lstat and ", ii))})

# this bit of code is still a work in progress, the lable is not coming through properly 
```


(b)	Fit a multiple regression model to predict the response using all the predictors. Describe your results. For which predictors can we reject the null hypothesis H0: b_j = 0?
*all except chas and nox.*

```{r}
#multiple linear regression achieved by using the shorthand 'lstat~.' meaning y ~ all predictors. 
lm_multi <- lm(lstat~.,data=Boston)
summary(lm_multi)
```



(c)	How do your results from (a) compare to your results from (b)?

*crim, nox, and tax were previously significant, now have p >.05*

Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression
model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.

```{r}
coef_multi <- data.frame(coef(lm_multi), check.rows = T, check.names = T, stringsAsFactors = T) 
coef_multi <- dplyr::filter(coef_multi, coef.lm_multi. < 35)
coef_multi <- dplyr::mutate(coef_multi, coef.lm_multi. = round(coef.lm_multi., digits = 4))

coef_simple <- data.frame(apply(Boston, 2, function(vv) coef(lm(lstat ~ vv, data = Boston))))
coef_simple <- data.frame(t(coef_simple))
coef_simple <- dplyr::select(coef_simple, vv)
coef_simple <- dplyr::mutate(coef_simple, vv = round(vv, digits = 4))
coef_simple <- dplyr::filter(coef_simple, vv != 1.0000)
coef_df <- bind_cols(coef_multi, coef_simple)

ggplot(data = coef_df, aes(x=vv, y=coef.lm_multi.)) + geom_point()+ labs(title = "simple vs multivariate linear regression coefficients", y="multivariate coefficients", x="simple coefficients") 



```


(d)	Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor rm, age and medv, fit a model of the form y=XB + e. 

* Yes. Each of them have F values >1 (though weak/modest for rm and age) and pvalues less than .05. *

```{r}
# use form as specified above.  
summary(lm(lstat~poly(rm + exp(1)), data = Boston))
# use orthoginal polynomial function mentioned in chapter 7
summary(lm(lstat~poly(rm, 2), data = Boston))

# chapter 3 lab uses the I function to do non-linear transformation
rm_lm_fit1 <- lm(lstat~rm, data = Boston)
rm_lm_fit2 <- lm(lstat~rm + I(rm^2), data = Boston)
# and anova comparison of the model without and with the non-linear component
anova(rm_lm_fit1, rm_lm_fit2)

age_lm_fit1 <- lm(lstat~age, data = Boston)
age_lm_fit2 <- lm(lstat~age + I(age^2), data = Boston)
anova(age_lm_fit1, age_lm_fit2)

medv_lm_fit1 <- lm(lstat~medv, data = Boston)
medv_lm_fit2 <- lm(lstat~medv + I(medv^2), data = Boston)
anova(medv_lm_fit1, medv_lm_fit2)

```


